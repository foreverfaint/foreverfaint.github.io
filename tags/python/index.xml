<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Python on Dev66</title><link>https://foreverfaint.github.io/tags/python/</link><description>Recent content in Python on Dev66</description><generator>Hugo -- gohugo.io</generator><language>zh-Hant</language><copyright>&amp;copy; Copyright notice</copyright><lastBuildDate>Fri, 11 Dec 2015 15:27:56 +0800</lastBuildDate><atom:link href="https://foreverfaint.github.io/tags/python/index.xml" rel="self" type="application/rss+xml"/><item><title>在Scrapy中使用cookie</title><link>https://foreverfaint.github.io/posts/scrapy-cookie/</link><pubDate>Fri, 11 Dec 2015 15:27:56 +0800</pubDate><guid>https://foreverfaint.github.io/posts/scrapy-cookie/</guid><description>&lt;p>Python有一个很出色的爬虫包&lt;a href="http://scrapy.org/">scrapy&lt;/a>，架构清晰，设计精巧，能想到的爬虫工具需要的定制化点都有对应的扩展机制。 大部分网站都使用cookie来记录访问用户的识别信息。每个请求都会把用户识别信息带回到服务器，帮助后台程序识别独立用户，这样可以进行鉴权，反爬，限流等很多的操作。所以对于爬虫来说，如何模拟和使用cookie“欺骗”服务器，是十分重要的一步。本文就介绍如何在scrapy中使用cookie技术。&lt;/p></description></item><item><title>Streaming Pipeline in Python - 1</title><link>https://foreverfaint.github.io/posts/streaming-pipeline-in-python-1/</link><pubDate>Sun, 01 Nov 2015 00:00:00 +0800</pubDate><guid>https://foreverfaint.github.io/posts/streaming-pipeline-in-python-1/</guid><description>&lt;p>最近用python 2.7做数据处理。数据说大不大，说小不小，千万级别。显然用Hadoop是大材小用。可由于每笔数据都是一个很大的json对象，处理起来很耗内存。单机加到8GB，依旧会出现OOM。不过还好此类问题有成熟的解决方案“流水线式的数据处理”：每次从文件读一笔记录数据，处理一笔数据，把处理结果持久化，相应的对象实例（内存）被回收。方案成熟易实现。先把代码列在下面，然后再解释其中遇到的坑。&lt;/p></description></item></channel></rss>